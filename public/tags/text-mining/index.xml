<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Text mining on Christian Ryan</title>
    <link>https://drchristianryan.com/tags/text-mining/</link>
    <description>Recent content in Text mining on Christian Ryan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Jan 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://drchristianryan.com/tags/text-mining/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using tidytext to compare samples of dreams</title>
      <link>https://drchristianryan.com/post/using-tidytext-to-compare-samples-of-dreams/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://drchristianryan.com/post/using-tidytext-to-compare-samples-of-dreams/</guid>
      <description>


&lt;p&gt;This is the third post in the series exploring text analytics with data from the dreambank.com. In the first post ‘Pulling text data from the internet’, I demonstrated how to use the &lt;strong&gt;rvest&lt;/strong&gt; package to pull text data from the dreambank website. In the second post ‘Manipulating text data from dreams’ we saw how to turn the dream texts into a tidy format by unnesting the word tokens in each dream and running counts on the word frequencies. In this third post of the series, I am going to demonstrate some ways of comparing texts from Julia Silge and David Robinson’s book &lt;strong&gt;Text Mining with R - A tidy approach&lt;/strong&gt; using the dream data set to illustrate the ideas, while also unpacking some of the steps a little further than in their book.&lt;/p&gt;
&lt;p&gt;We will load our packages again and pull in the same data we analysed last time (see the previous post for details).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(stringr)
library(car)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The processing steps we used last time on the data was to add a dream number, unnest the tokens, remove stopwords, and then filter out underscores and digits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word &amp;lt;- df %&amp;gt;%
  mutate(dream_number = row_number()) %&amp;gt;%
  unnest_tokens(word, dream) %&amp;gt;%
  anti_join(stop_words) %&amp;gt;%
  filter(str_detect(word, pattern = &amp;quot;_&amp;quot;, negate = TRUE)) %&amp;gt;%
  filter(str_detect(word, pattern = &amp;#39;[\\d]&amp;#39;, negate = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we calculated word frequencies as proportions. This time we will store this as a new dataframe called df_proportion. Notice we have to remove the temporary ‘n’ variable. I am not sure why this is, but if you don’t deselect it, it seems to mess with the spread() function, creating multiple rows for the same word.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_proportion &amp;lt;- df_word %&amp;gt;%
  group_by(sample, word) %&amp;gt;%
  summarise(n = n()) %&amp;gt;%
  mutate(percent = (n / sum(n))*100) %&amp;gt;%
  mutate(percent = round(percent, 2)) %&amp;gt;%
  select(-n) %&amp;gt;%
  arrange(desc(percent)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;comparing-word-frequencies-across-samples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing word frequencies across samples&lt;/h1&gt;
&lt;p&gt;We might want to compare the frequencies across samples. We can use a technique that Juile Silge and David Robinson used to compare word frequencies across authors. This is a clever trick in which they use the &lt;em&gt;spread()&lt;/em&gt; and &lt;em&gt;gather()&lt;/em&gt; functions. Spread gives each sample their own column and makes the value proportion. Rather than do the spread and gather in one code chuck as in the book, I will do them as two separate stages to illustrate the process in more detail.&lt;/p&gt;
&lt;p&gt;So let’s spread the data first. Think of this as taking the column of percentages/proportions and moving it into four separate columns: one for each sample. We are passing two arguments to the &lt;em&gt;spread()&lt;/em&gt; function, the key (which contains the names of the items to form new columns) which is our ‘sample’ variable, and the value (what will the cells in each of these columns be filled with), which is the variable ‘percent’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_spread &amp;lt;- df_proportion %&amp;gt;%
  spread(sample, percent)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could look at this new dataframe directly by clicking on the icon in the Global Environment, but you will notice lots of NA values. This is because each dream sample contains words that are unique to that sample and in a way these are the least informative - we can only make a binary comparison if one sample contains the word and the other does not. So we really want to see the variance in proportions for words that occur in more than one sample. To take a better look at these examples we can run our new dataframe called df_spread through a !is.na (is not NA) filter and then call the &lt;em&gt;some()&lt;/em&gt; function from the &lt;strong&gt;car&lt;/strong&gt; package. This is simply filtering out the rows in which the NA value occurs for the proportion of a word in any of our four samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_spread %&amp;gt;%
  filter(!is.na(college_women) &amp;amp; !is.na(hall_female) &amp;amp; !is.na(hall_male) &amp;amp; !is.na(vietnam_vet)) %&amp;gt;%
  some()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 5
##    word       college_women hall_female hall_male vietnam_vet
##    &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 apparently          0.15        0.02      0.09        0.02
##  2 beach               0.06        0.07      0.07        0.02
##  3 bedroom             0.12        0.12      0.09        0.13
##  4 charge              0.03        0.05      0.02        0.05
##  5 cross               0.06        0.02      0.02        0.03
##  6 distance            0.03        0.1       0.09        0.14
##  7 lost                0.03        0.07      0.07        0.05
##  8 short               0.09        0.05      0.19        0.17
##  9 single              0.03        0.02      0.05        0.03
## 10 walls               0.03        0.02      0.02        0.09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then the next step is to decide which sample is going to be our reference sample. This will have its column of frequencies replicated and stacked so that each of the other samples can be compared with it. When we &lt;em&gt;gather()&lt;/em&gt;, we only include the samples to compare with the reference and not the reference itself. For simplicity, we can use the first column in our df (college_women) as our reference group and compare the other three samples to this. We might predict at this point that the hall_women will have the most similar dreams and then the hall_men, with the vietnam_vet being the most different. We can check this prediction later on when we run some correlation coefficients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_gather &amp;lt;- df_spread %&amp;gt;%
  gather(sample, proportion, hall_female:vietnam_vet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a quick look at the structure of the output, but we will filter out the NA values first, as we did for the df_spread dataset, as many are created for words that appear in one sample of dreams but not the other. Notice we do the same not-NA process (!is.na) on both the college_women data and the “proportion” variable. Remember that the college_women are our reference sample, so the ‘sample’ in this case tells us who provide the data in the proportion column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_gather %&amp;gt;%
  filter(!is.na(college_women) &amp;amp; !is.na(proportion))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,950 x 4
##    word     college_women sample      proportion
##    &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;
##  1 accident          0.03 hall_female       0.02
##  2 act               0.06 hall_female       0.05
##  3 afraid            0.24 hall_female       0.1 
##  4 age               0.61 hall_female       0.27
##  5 aged              0.09 hall_female       0.02
##  6 ages              0.03 hall_female       0.05
##  7 ago               0.12 hall_female       0.15
##  8 ahead             0.06 hall_female       0.02
##  9 air               0.15 hall_female       0.02
## 10 alarm             0.09 hall_female       0.02
## # … with 1,940 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we can graph this data to inspect it more effectively. We use the df_gather as our data and set the aesthetics for x and y to proportion (which are our three samples) and college_women - which is our reference sample. As the college_women data will be used in all three graphs, it makes sense to assign it to the y axis, so we can scan down all three graphs vertically to make comparisons. There is another clever trick here with the colour variable. We will set it to the absolute value - &lt;em&gt;abs()&lt;/em&gt; - of the difference between the college_women value and the other sample (proportion) value. This means that the darker the colour of the dot, the stronger the difference between the two proportion values. This allows us to use both shade (dark to light) and position away from the diagonal line as a measure of difference between samples. The paler the text the more similar the proportion use of the word in both samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(scales)

ggplot(df_gather, aes(x = proportion, y = college_women,
                 colour = abs(college_women - proportion)))+
  geom_abline(colour = &amp;quot;gray40&amp;quot;, lty = 2)+
  geom_jitter(alpha = 0.2, size = 2, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 2)+
  scale_x_log10(labels = percent_format())+
  scale_y_log10(labels = percent_format())+
  scale_color_gradient(limits = c(0, 0.22),
                       low = &amp;quot;grey&amp;quot;, high = &amp;quot;black&amp;quot;)+
  facet_wrap(~sample, ncol = 1)+
  theme(legend.position = &amp;quot;none&amp;quot;)+
  labs(y = &amp;quot;college_women&amp;quot;, x = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-01-27-using-tidytext-to-compare-samples-of-dreams_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The words on or close to the line represent words that occurred at similar frequencies in both samples. As an example, we can see the word ‘remember’ occurred at high frequency in the first graph, in both the college_women and hall_female samples, as it is both high up and far to the right on the graph - but we can also observe that the closeness to the line suggests a similar frequency between the samples. In contrast, we can see on the same graph that the word “aunt” is quite far to the left of the line, indicating that it occurs more frequently in the college_women than the hall_women. This is also the case in the graph of college_women against hall_men, whereas this is not obvious in the final graph. It is possible that the word doesn’t occur at all in the vietnam_vet dream - therefore it would have been removed by graph function call. We can check the values for ‘aunt’ with a quick call of the df_spread data.frame (this is why it can be useful to keep both versions, rather than overwriting the data when we used the gather function).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_spread %&amp;gt;%
  filter(word == &amp;quot;aunt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 5
##   word  college_women hall_female hall_male vietnam_vet
##   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 aunt           0.43        0.02      0.02          NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we predicted, the lack of ‘aunt’ in the final graph was not due to a similar frequency between the samples, but rather the complete absence of the word in the vietnam_vet dreams. We can also see the size of difference between the frequency of the word in the college_women’s dreams and the other three samples is very large. Graphing the words in this way can give you a strong sense of these differences between the texts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-coefficient&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation coefficient&lt;/h1&gt;
&lt;p&gt;We can use the base R function cor.test() to measure the degree of similarity between the propotions of words used in each sample. One approach is to prepare a new dataframe to carry out this test. Here we create a dataframe called df_cwhf(college_women and hall_female) and filter for just those rows that represent data for college women and hall_females. We can then run the &lt;em&gt;cor.test()&lt;/em&gt; on this dataframe, by passing the variables ‘college_women’ and ‘proportion’ - the latter being just the proportion for the hall_female (because of the filter we applied).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_cwhf &amp;lt;- df_gather %&amp;gt;%
  filter(sample == &amp;quot;hall_female&amp;quot;)
cor.test(df_cwhf$college_women, df_cwhf$proportion)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  df_cwhf$college_women and df_cwhf$proportion
## t = 34.676, df = 643, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7785169 0.8325231
## sample estimates:
##       cor 
## 0.8072027&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, we could use the syntax in &lt;strong&gt;Text mining with R&lt;/strong&gt;, which subsets the data on the fly. We explicitly give a data argument, which is the df_gather dataframe, subsetted with &lt;em&gt;samples == “hall_female”&lt;/em&gt;. We add a comma after this in the square brackets &lt;strong&gt;[ ]&lt;/strong&gt;, with no other argument, as we want to retain all the columns. Finally we provide our x and y values to be correlated in the formula format &lt;strong&gt;~ x + y&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test(data = df_gather[df_gather$sample == &amp;quot;hall_female&amp;quot;, ], ~ proportion + college_women)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  proportion and college_women
## t = 34.676, df = 643, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7785169 0.8325231
## sample estimates:
##       cor 
## 0.8072027&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s use this one-line technique to run the other two comparisons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test(data = df_gather[df_gather$sample == &amp;quot;hall_male&amp;quot;, ], ~ proportion + college_women)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  proportion and college_women
## t = 28.24, df = 648, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7062132 0.7753868
## sample estimates:
##       cor 
## 0.7427756&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test(data = df_gather[df_gather$sample == &amp;quot;vietnam_vet&amp;quot;, ], ~ proportion + college_women)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  proportion and college_women
## t = 12.03, df = 653, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.3610922 0.4866471
## sample estimates:
##      cor 
## 0.425918&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This reveals that all three samples correlate highly with the college_women, but the vietnam_vet dreams had a much lower correlation (.43) than the hall_female (.81) or the hall_male (.74). We could go on to use term frequency–inverse document frequency (tf-idf) to make a more complex comparison between the samples. In the next post, we will apply some of the sentiment analysis ideas from the book to the dream data. Let’s save our dataset df_word for next time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;save(df_word, file = &amp;quot;df_word.Rdata&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Manipulating text data from dreams</title>
      <link>https://drchristianryan.com/post/manipulating-text-data-from-dreams/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://drchristianryan.com/post/manipulating-text-data-from-dreams/</guid>
      <description>


&lt;p&gt;In the previous post on ‘pulling text data from the internet’, I experimented with pulling out the dream text from a sample of dreams from the website “DreamBank” at: &lt;a href=&#34;http://www.dreambank.net/random_sample.cgi&#34; class=&#34;uri&#34;&gt;http://www.dreambank.net/random_sample.cgi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this follow-up post, I will demonstrate some of the methods presented in Julia Silge and David Robinson’s book ‘Text Mining with R’ for processing text data, as applied to 400 dreams sampled from 4 collections in the dreambank. I used the methods described in the last post to pull out a random sample of 100 dreams from each of the following 4 groups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;college_women (this was the sample used last time)&lt;/li&gt;
&lt;li&gt;hall_female&lt;/li&gt;
&lt;li&gt;hall_male&lt;/li&gt;
&lt;li&gt;vietnam_vet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first set of dreams were recorded by college women by Calvin Hall from undergraduates in a course on personality at Western Reserve University in 1947 and 1948.&lt;/p&gt;
&lt;p&gt;The second and third samples are also dreams collected by Calvin Hall and Robert L. Van de Castle, on which they based female and male norms in their book &lt;em&gt;The Content Analysis of Dreams&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The sample listed as vietnam_vet are from the dreams of an American veteran of the Vietnam war, who suffered PTSD. The website has over 400 of his dreams which he donated from records he kept not long after returning from Vietnam.&lt;/p&gt;
&lt;p&gt;Let’s begin by loading the three packages we are likely to use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(stringr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to follow along with this post, the dataset I am about to load is “dream_df.csv”, which can be found on my github page: &lt;a href=&#34;https://github.com/Christian-Ryan/netsite/tree/master/public/post&#34; class=&#34;uri&#34;&gt;https://github.com/Christian-Ryan/netsite/tree/master/public/post&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- read_csv(&amp;quot;dreams_df.csv&amp;quot;)
df &amp;lt;- df[,2:3]
df$sample &amp;lt;- as.factor(df$sample)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After sampling the four dream sets, using the techniques described in the last post, we now have a dataframe called df with two variables - &lt;em&gt;sample&lt;/em&gt; and &lt;em&gt;dream&lt;/em&gt;. We will use our custom_view() function we created last time to display snippets of dreams neatly formatted. We can also use the &lt;em&gt;some()&lt;/em&gt; function from the car package to take a quick look at a selection of dreams across the dataframe. The &lt;em&gt;some()&lt;/em&gt; function is very like &lt;em&gt;head()&lt;/em&gt; and &lt;em&gt;tail()&lt;/em&gt;, but has the advantage of returning a selection across the dataset, which allows us to see examples from each of the samples simultaneously.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;custom_view &amp;lt;- function(x) data.frame(lapply(x, substr, 1, 56))
car::some(df) %&amp;gt;%
  custom_view()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           sample                                                    dream
## 1  college_women I was at a factory working. I saw a college girl-friend 
## 2  college_women A friend of mine (male) came over and came into the bedr
## 3  college_women I was walking up my street when I heard the wild barking
## 4    vietnam_vet I&amp;#39;m with my old platoon on a firebase in Vietnam. An FNG
## 5      hall_male This took place in the South Pacific aboard a destroyer.
## 6      hall_male This dream has been recurrent at the rate of twice a wee
## 7    hall_female I dreamt that a friend of mine who graduated last year c
## 8    hall_female All I remember of this dream is that my brother (16) re-
## 9    hall_female I dreamed that I was sitting in the back room of Moe&amp;#39;s, 
## 10   hall_female My father and mother had become parents of a baby boy. I&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/tidytext.png&#34; width=&#34;25%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Julia Silge and David Robinson’s book &lt;strong&gt;Text Mining with R - A tidy approach&lt;/strong&gt; sets off at a cracking pace, at least for relatively newbies to R such as myself. They assumes a degree of familiarity with tidyverse concepts and when they introduce concepts such as tidytext format, they can sometimes address three or four steps in one example. I will unpack some of these as individual steps to illustrate what is going on, while using our dream data as the material for processing.&lt;/p&gt;
&lt;p&gt;At the moment our df only contains the sample name (a categorical variable with four values) and the text of the dream. It might be helpful to index the dreams before we tokenise the text in them. So let’s introduce a new variable that we will call dream_number. This will index each dream between 1 - 400 in the dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;-  df %&amp;gt;%
  mutate(dream_number = row_number())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have the dream_number variable added, we can unnest the tokens (split the text variable into individual words). The syntax for the unnest_tokens() function is to pipe in the dataframe (df), then supply the name of the variable to be created (word), followed by the variable containing the text we are going to tokenise - in this case “dream”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word &amp;lt;- df %&amp;gt;%
  unnest_tokens(word, dream)
head(df_word)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   sample        dream_number word   
##   &amp;lt;fct&amp;gt;                &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  
## 1 college_women            1 i      
## 2 college_women            1 dreamed
## 3 college_women            1 that   
## 4 college_women            1 i      
## 5 college_women            1 was    
## 6 college_women            1 in&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See that the &lt;em&gt;word&lt;/em&gt; variable has replaced our &lt;em&gt;dream&lt;/em&gt; variable and now each word is on a separate row - this is the tidytext format. &lt;em&gt;unnest_tokens()&lt;/em&gt; has kept the variables &lt;em&gt;sample&lt;/em&gt; and &lt;em&gt;dream_number&lt;/em&gt; - it only transforms the input variable (dream) into the output variable (word). Notice also that the function has transformed into lower-case all the words in the &lt;em&gt;word&lt;/em&gt; variable.&lt;/p&gt;
&lt;div id=&#34;tokenisation-and-n-grams&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tokenisation and N-Grams&lt;/h1&gt;
&lt;div id=&#34;section&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;&lt;/h6&gt;
&lt;p&gt;It should be noted that when we use unnest_tokens() we are using a range of default values. We could have specified something other than single words in our output. The default value of the token argument is ‘word’. We can change this to ‘ngram’ and use an ‘n=’ to specify how many words should be kept as a group. Let us try a quick run with 3-word tokens instead of single words to demonstrate this behaviour.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_trigrams &amp;lt;- df %&amp;gt;%
  unnest_tokens(trigrams, dream, token = &amp;quot;ngrams&amp;quot;, n = 3)
head(df_trigrams)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   sample        dream_number trigrams      
##   &amp;lt;fct&amp;gt;                &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;         
## 1 college_women            1 i dreamed that
## 2 college_women            1 dreamed that i
## 3 college_women            1 that i was    
## 4 college_women            1 i was in      
## 5 college_women            1 was in the    
## 6 college_women            1 in the office&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here we have set our output variable to ‘trigrams’ and specified the token argument to be equal to ‘ngrams’, and we have saved this as a new dataframe called ‘df_trigrams’. That gives us a better sense of the nature of the text. We can also run a count on this after grouping by sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_trigrams %&amp;gt;%
  group_by(sample) %&amp;gt;%
  count(trigrams, sort = TRUE) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 50,935 x 3
##    sample        trigrams              n
##    &amp;lt;fct&amp;gt;         &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;
##  1 vietnam_vet   i tell him           33
##  2 hall_female   i was in             29
##  3 college_women i was in             25
##  4 vietnam_vet   the scene changes    23
##  5 college_women and i was            22
##  6 hall_female   seemed to be         19
##  7 college_women that i was           18
##  8 hall_female   that i was           17
##  9 hall_male     seemed to be         17
## 10 hall_female   and i was            16
## # … with 50,925 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see that in the Vietnam veteran dream sample, the most common three word phrase was “I tell him”, whereas for the Hall Female and College Women the most common phrase was “I was in”. Using ngrams (units larger than one word), can be useful in exploring most frequently occurring phrases. It is notable that the phrase for the Vietnam vet was in the present tense, giving a sense of the immediacy and immersion of the dream experience, whereas those most frequent phrases of the other samples are in the past tense.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;single-words-bag-of-words-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Single words (Bag of words approach)&lt;/h1&gt;
&lt;p&gt;We have not removed stop-words yet as this would undermine our exploration of ngrams. But this is the next step for our df_word dataset. The &lt;em&gt;anti_join()&lt;/em&gt; function, takes two dataframes and keeps only those words that don’t occur in both dataframes. So this forms a convenient and easy way to filter out unwanted stopwords.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word &amp;lt;- df_word %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can count the words and sort them into descending order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  count(word, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5,331 x 2
##    word         n
##    &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
##  1 house      133
##  2 dream      132
##  3 remember   125
##  4 car        118
##  5 people     110
##  6 girl       108
##  7 friend     101
##  8 time        95
##  9 woman       93
## 10 mother      85
## # … with 5,321 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But before we create some plots of these words, we should check for any anomalies in the &lt;em&gt;word&lt;/em&gt; variable of df_word. The sorted count is likely to give back expected results (high frequency genuine words). But there can be other text elements that we may want to filter out. This will become obvious if we count, but don’t sort.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  count(word)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5,331 x 2
##    word       n
##    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;
##  1 ___        1
##  2 ______     1
##  3 00         2
##  4 1          4
##  5 1,500      1
##  6 10        13
##  7 100        3
##  8 105        1
##  9 107th      1
## 10 109        1
## # … with 5,321 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;word&lt;/em&gt; variable contains some text elements that we would not regard as words. Let’s check where the underscores came from. To do this we must go back to our original (untokenised) dataset &lt;em&gt;df&lt;/em&gt;, as we want to see the underscores in the context of the dream. We can use the &lt;em&gt;str_which()&lt;/em&gt; function to identify which dreams contain underscores, matched to the pattern &lt;code&gt;&#39;___&#39;&lt;/code&gt;. Then we can use this as an index on the df$dream variable, so that it just returns the context of the dreams with underscores. As there are three dreams with underscores, we will store this sequence of dreams and then take a look at the first one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;underscores &amp;lt;- df$dream[str_which(df$dream, pattern = &amp;quot;___&amp;quot;)]
underscores[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;I dreamed about a young married couple whom I have known for a long time. They came to see us at our home. Although the home was ours, it resembled my Uncle&amp;#39;s home in C___ and yet the dream seemed to take place in C ___.. They drove up in a Model A Ford &amp;amp; parked it in the front yard. We were in the living room talking when another Model A Ford drove up &amp;amp; in it were my sister &amp;amp; a friend of mine. I went out in the front yard, got in this couple&amp;#39;s car, and started to talk to my sister. D___ my sister, asked me if I wanted to go to a play with J. She said that she and her husband weren&amp;#39;t going. I realized that I would have to go with him alone, so I refused. Then they drove away and the wife came out in the yard. She seemed perturbed at my getting into their car, so she got into the car and backed it away. The car then suddenly changed into an old-fashioned bicycle. It was at this time that I felt antagonistic towards this couple.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the pattern here seems to be that underscores are used to disguise the identity of named people in the dreams. We can choose to filter these out as they are not relevant to our analysis. But before we do this filtering, let’s also consider the numbers in the &lt;em&gt;word&lt;/em&gt; variable column - again in a bag-of-words approach one could argue that these are not words and so are irrelevant. We want to create a pattern that identifies both digits and underscores, and then use a function to transform our &lt;em&gt;word&lt;/em&gt; variable in the df_word dataframe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-pattern-to-remove-numbers-and-underscores&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Create pattern to remove numbers and underscores&lt;/h1&gt;
&lt;p&gt;We can use the function str_subset() to identify the elements of the &lt;em&gt;word&lt;/em&gt; variable that we wish to remove. Let’s create a pattern that deals initially with the underscores and try &lt;em&gt;str_subset()&lt;/em&gt; with it. The ‘+’ is not strictly necessary here, but it illustrates that we can identify at least one underscore by this combination.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_subset(df_word$word, pattern = &amp;#39;_+&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;n__&amp;quot;    &amp;quot;y__&amp;quot;    &amp;quot;c___&amp;quot;   &amp;quot;___&amp;quot;    &amp;quot;d___&amp;quot;   &amp;quot;h___&amp;quot;   &amp;quot;a___&amp;quot;   &amp;quot;a___&amp;quot;  
##  [9] &amp;quot;h__&amp;quot;    &amp;quot;______&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has found ten instances of the underscore in the word variable. Now we want to find all the digits. We could use the regex shorthand &lt;em&gt;[\d]&lt;/em&gt; or &lt;em&gt;[:digit:]&lt;/em&gt;. Let’s use the latter first with &lt;em&gt;str_subset&lt;/em&gt; to check it works.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_subset(df_word$word, pattern = &amp;#39;[:digit:]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;169&amp;quot;    &amp;quot;80&amp;quot;     &amp;quot;90&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;60&amp;quot;     &amp;quot;40&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;4&amp;quot;     
##   [9] &amp;quot;20&amp;quot;     &amp;quot;4&amp;quot;      &amp;quot;5&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;34&amp;quot;     &amp;quot;34&amp;quot;     &amp;quot;309&amp;quot;    &amp;quot;219&amp;quot;   
##  [17] &amp;quot;6&amp;quot;      &amp;quot;5.00&amp;quot;   &amp;quot;8&amp;quot;      &amp;quot;5&amp;quot;      &amp;quot;45&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;22&amp;quot;    
##  [25] &amp;quot;50&amp;quot;     &amp;quot;23&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;22&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;22&amp;quot;     &amp;quot;11&amp;quot;    
##  [33] &amp;quot;8&amp;quot;      &amp;quot;8&amp;quot;      &amp;quot;12&amp;quot;     &amp;quot;3rd&amp;quot;    &amp;quot;26&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;22&amp;quot;    
##  [41] &amp;quot;60&amp;quot;     &amp;quot;70&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;25&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;27&amp;quot;     &amp;quot;23&amp;quot;     &amp;quot;52&amp;quot;    
##  [49] &amp;quot;23&amp;quot;     &amp;quot;7&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;2nd&amp;quot;    &amp;quot;2nd&amp;quot;    &amp;quot;7&amp;quot;      &amp;quot;10&amp;quot;     &amp;quot;10&amp;quot;    
##  [57] &amp;quot;2&amp;quot;      &amp;quot;1&amp;quot;      &amp;quot;1&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;999&amp;quot;    &amp;quot;e1&amp;quot;     &amp;quot;10&amp;quot;     &amp;quot;2&amp;quot;     
##  [65] &amp;quot;4&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;25&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;5&amp;quot;      &amp;quot;22&amp;quot;    
##  [73] &amp;quot;20&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;6&amp;quot;      &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;    
##  [81] &amp;quot;50&amp;quot;     &amp;quot;4&amp;quot;      &amp;quot;35&amp;quot;     &amp;quot;4&amp;quot;      &amp;quot;00&amp;quot;     &amp;quot;40&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;5&amp;quot;     
##  [89] &amp;quot;10&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;80&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;48&amp;quot;     &amp;quot;55&amp;quot;     &amp;quot;22&amp;quot;     &amp;quot;40&amp;quot;    
##  [97] &amp;quot;1992&amp;quot;   &amp;quot;200&amp;quot;    &amp;quot;300&amp;quot;    &amp;quot;100&amp;quot;    &amp;quot;20s&amp;quot;    &amp;quot;30s&amp;quot;    &amp;quot;1950s&amp;quot;  &amp;quot;2001&amp;quot;  
## [105] &amp;quot;2012&amp;quot;   &amp;quot;10&amp;quot;     &amp;quot;12&amp;quot;     &amp;quot;1990s&amp;quot;  &amp;quot;50s&amp;quot;    &amp;quot;1972&amp;quot;   &amp;quot;1950s&amp;quot;  &amp;quot;800&amp;quot;   
## [113] &amp;quot;45&amp;quot;     &amp;quot;60s&amp;quot;    &amp;quot;1970&amp;quot;   &amp;quot;45&amp;quot;     &amp;quot;1960s&amp;quot;  &amp;quot;105&amp;quot;    &amp;quot;1st&amp;quot;    &amp;quot;109&amp;quot;   
## [121] &amp;quot;110&amp;quot;    &amp;quot;116&amp;quot;    &amp;quot;121&amp;quot;    &amp;quot;122&amp;quot;    &amp;quot;2001&amp;quot;   &amp;quot;2012&amp;quot;   &amp;quot;138&amp;quot;    &amp;quot;139&amp;quot;   
## [129] &amp;quot;152&amp;quot;    &amp;quot;m16&amp;quot;    &amp;quot;m60&amp;quot;    &amp;quot;59&amp;quot;     &amp;quot;2001&amp;quot;   &amp;quot;2012&amp;quot;   &amp;quot;39&amp;quot;     &amp;quot;244&amp;quot;   
## [137] &amp;quot;1200&amp;quot;   &amp;quot;207&amp;quot;    &amp;quot;208&amp;quot;    &amp;quot;209&amp;quot;    &amp;quot;211&amp;quot;    &amp;quot;214&amp;quot;    &amp;quot;215&amp;quot;    &amp;quot;216&amp;quot;   
## [145] &amp;quot;800&amp;quot;    &amp;quot;411&amp;quot;    &amp;quot;42nd&amp;quot;   &amp;quot;217&amp;quot;    &amp;quot;218&amp;quot;    &amp;quot;219&amp;quot;    &amp;quot;2am&amp;quot;    &amp;quot;123&amp;quot;   
## [153] &amp;quot;220&amp;quot;    &amp;quot;1950s&amp;quot;  &amp;quot;2&amp;quot;      &amp;quot;20&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;19&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;22&amp;quot;    
## [161] &amp;quot;8&amp;quot;      &amp;quot;27&amp;quot;     &amp;quot;3&amp;quot;      &amp;quot;1,500&amp;quot;  &amp;quot;50&amp;quot;     &amp;quot;17&amp;quot;     &amp;quot;26&amp;quot;     &amp;quot;30&amp;quot;    
## [169] &amp;quot;10&amp;quot;     &amp;quot;70&amp;quot;     &amp;quot;6&amp;quot;      &amp;quot;3&amp;quot;      &amp;quot;4&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;33&amp;quot;     &amp;quot;45&amp;quot;    
## [177] &amp;quot;4&amp;quot;      &amp;quot;12&amp;quot;     &amp;quot;12&amp;quot;     &amp;quot;160&amp;quot;    &amp;quot;10&amp;quot;     &amp;quot;11&amp;quot;     &amp;quot;85&amp;quot;     &amp;quot;22&amp;quot;    
## [185] &amp;quot;11&amp;quot;     &amp;quot;10&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;300&amp;quot;    &amp;quot;30&amp;quot;     &amp;quot;10&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;440&amp;quot;   
## [193] &amp;quot;880&amp;quot;    &amp;quot;10&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;3000&amp;quot;   &amp;quot;3&amp;quot;      &amp;quot;3&amp;quot;      &amp;quot;3&amp;quot;     
## [201] &amp;quot;11&amp;quot;     &amp;quot;12&amp;quot;     &amp;quot;12&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;13&amp;quot;     &amp;quot;26&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;    
## [209] &amp;quot;11&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;19&amp;quot;     &amp;quot;7&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;28&amp;quot;    
## [217] &amp;quot;50&amp;quot;     &amp;quot;30&amp;quot;     &amp;quot;18&amp;quot;     &amp;quot;18&amp;quot;     &amp;quot;15&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;21&amp;quot;     &amp;quot;20&amp;quot;    
## [225] &amp;quot;6&amp;quot;      &amp;quot;30&amp;quot;     &amp;quot;19&amp;quot;     &amp;quot;16&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;23&amp;quot;     &amp;quot;25&amp;quot;     &amp;quot;35&amp;quot;    
## [233] &amp;quot;40&amp;quot;     &amp;quot;25&amp;quot;     &amp;quot;5&amp;quot;      &amp;quot;3&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;23&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;3&amp;quot;     
## [241] &amp;quot;3&amp;quot;      &amp;quot;1&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;23&amp;quot;     &amp;quot;40&amp;quot;     &amp;quot;35&amp;quot;     &amp;quot;8&amp;quot;     
## [249] &amp;quot;8&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;107th&amp;quot;  &amp;quot;16&amp;quot;     &amp;quot;27&amp;quot;     &amp;quot;10&amp;quot;     &amp;quot;8&amp;quot;      &amp;quot;60&amp;quot;    
## [257] &amp;quot;21&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;11&amp;quot;     &amp;quot;15&amp;quot;     &amp;quot;11&amp;quot;     &amp;quot;17&amp;quot;    
## [265] &amp;quot;17&amp;quot;     &amp;quot;2&amp;quot;      &amp;quot;34&amp;quot;     &amp;quot;45&amp;quot;     &amp;quot;49&amp;quot;     &amp;quot;52&amp;quot;     &amp;quot;55&amp;quot;     &amp;quot;3&amp;quot;     
## [273] &amp;quot;5&amp;quot;      &amp;quot;20&amp;quot;     &amp;quot;26&amp;quot;     &amp;quot;75.00&amp;quot;  &amp;quot;2&amp;quot;      &amp;quot;6&amp;quot;      &amp;quot;27&amp;quot;     &amp;quot;3&amp;quot;     
## [281] &amp;quot;4&amp;quot;      &amp;quot;00&amp;quot;     &amp;quot;1st&amp;quot;    &amp;quot;2nd&amp;quot;    &amp;quot;3rd&amp;quot;    &amp;quot;10&amp;quot;     &amp;quot;20&amp;quot;     &amp;quot;10&amp;quot;    
## [289] &amp;quot;30&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;50&amp;quot;     &amp;quot;11,000&amp;quot; &amp;quot;1&amp;quot;      &amp;quot;48th&amp;quot;   &amp;quot;4&amp;quot;      &amp;quot;6&amp;quot;     
## [297] &amp;quot;25th&amp;quot;   &amp;quot;100&amp;quot;    &amp;quot;100&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This works very nicely as well. However, to use these patterns with the tidyverse pipe, it is easier to use the &lt;em&gt;fitler()&lt;/em&gt; function rather than &lt;em&gt;str_subset()&lt;/em&gt;, and since it is convenient to chain steps in the pipe, we can use two calls to &lt;em&gt;filter()&lt;/em&gt;, first by underscores and secondly by digits. And as we don’t want either of these in our dataset, we will set the “negate” argument to TRUE in both cases. An alternative method to delete the digits would be to use the capital “D” in the regex, but this way keeps our filters more uniform, both with a “negate = TRUE” argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  filter(str_detect(word, pattern = &amp;quot;_&amp;quot;, negate = TRUE)) %&amp;gt;%
  filter(str_detect(word, pattern = &amp;#39;[\\d]&amp;#39;, negate = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17,953 x 3
##    sample        dream_number word      
##    &amp;lt;fct&amp;gt;                &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     
##  1 college_women            1 dreamed   
##  2 college_women            1 office    
##  3 college_women            1 directress
##  4 college_women            1 nurses    
##  5 college_women            1 nursing   
##  6 college_women            1 school    
##  7 college_women            1 forty     
##  8 college_women            1 told      
##  9 college_women            1 results   
## 10 college_women            1 i.q       
## # … with 17,943 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-word-frequencies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plot word frequencies&lt;/h1&gt;
&lt;p&gt;Now we have done some tidying on the dataset, we can plot the word frequencies - a simple way is to pass them through a filter so we only retain those words with a frequency greater than say n = 60. Notice we use mutate to create the new variables for the plot &lt;em&gt;word&lt;/em&gt; (in the order of frequency) and &lt;em&gt;n&lt;/em&gt;. We then filter by frequency, and pass the two new variables to the ggplot function. We also have to switch syntax at this point from the pipe ( %&amp;gt;% ) to the + sign between the layers of the ggplot() function. We flip the coordinates, as it allows us to keep the words in the horizontal aspect and makes it the plot easier to read.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  count(word, sort = TRUE) %&amp;gt;%
  mutate(word = reorder(word, n)) %&amp;gt;%
  filter(n &amp;gt; 60) %&amp;gt;%
  ggplot(aes(x = word, y = n)) +
  geom_col()+
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-01-14-manipulating-text-data-from-dreams_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This gives us an overview of the most commonly used words in dreams recalled by all four samples. But it would be more interesting to see how the word use differs between the samples. However, we should be prepared for the possibility that the length of dreams may vary between samples. To control for this, we might want to convert our raw counts of words to proportions from the dream text. Let’s check for the variety of dream lengths by using str_count() function on our original dataset df - hence before we removed our stopwords. We will count the words in each dream and store the result in a vector called dream_lengths. The default for &lt;em&gt;str_count&lt;/em&gt; is for the function to count characters if no pattern is given to match. However, if we pass it a second argument, specifying the regex for all sequences of non-space characters, it will count words instead. The regex includes the code for any non-white space character ‘\S’, with the addition of ‘+’ sign to indicate one or more non-white space characters, and the initial escape character ‘\’ as ‘\S’ is not recognised as an escape character without it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dream_lengths &amp;lt;- str_count(df$dream, &amp;quot;\\S+&amp;quot;)
plot(dream_lengths, xlab = &amp;quot;Dream Number&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-01-14-manipulating-text-data-from-dreams_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a good example of the use of the plot() function with a single vector in R. The default behaviour is to plot the values of the vector against the y-axis - dream_lengths in this case, and then use the index number (ie. the order in which each value occurs in the vector) as the x value. So our x-axis simple represents the order of the dreams, or as we have named this, the dream number. We can see here the range of dream lengths with the minimum being about 35 words and the maximum around 290 words. We could take the min, max, mean and SD if we wanted to be more specific.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(dream_lengths); max(dream_lengths); mean(dream_lengths); sd(dream_lengths)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 38&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 288&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 141.0325&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 45.09413&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a great deal of variability in the dream lengths, so proportions will be better than raw counts to represent the frequency of each word.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-word-frequencies-as-proportions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Calculating word frequencies as proportions&lt;/h1&gt;
&lt;p&gt;We will want to count proportions after stopwords are removed. We have a choice here whether we want to express the frequency of individual words by proportion of a dream or proportion of a sample. These would have different interpretations. If the texts (in our case dreams) were much longer, proportion by text might be the better way to represent the data, but I suspect proportion by dream may not be very informative. Let’s try it and see what the results look like. We will &lt;em&gt;group_by()&lt;/em&gt; dream_number so as to create proportion by dream. Then we use a summarise function to create a word count, and we use mutate to convert this to percentage. I used a second mutate to clean this up into two decimal places with the &lt;em&gt;round()&lt;/em&gt; function. Finally, we use the tidyverse equivalent of &lt;em&gt;sort()&lt;/em&gt; which is the &lt;em&gt;arrange()&lt;/em&gt; function - but because we want this to be largest-to-smallest, we also include the &lt;em&gt;desc()&lt;/em&gt; descending function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  group_by(dream_number, word) %&amp;gt;%
  summarise(n = n()) %&amp;gt;%
  mutate(percent = (n / sum(n))*100) %&amp;gt;%
  mutate(percent = round(percent, 2)) %&amp;gt;%
  arrange(desc(percent)) %&amp;gt;%
  ungroup&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15,500 x 4
##    dream_number word         n percent
##           &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1            6 remember     3    21.4
##  2          358 office       5    20  
##  3          355 dog          7    19.4
##  4          381 bus          6    18.2
##  5            2 hair         3    16.7
##  6           20 bed          3    16.7
##  7           83 store        5    16.7
##  8          260 car          6    16.7
##  9          399 test         6    15.8
## 10           13 dream        2    15.4
## # … with 15,490 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So in dream number 6 the word ‘remember’ accounted for 21% of the non-stopwords used. That seems like a high proportion. It might be more useful to look at the data aggregated across samples. We can change the code to group_by &lt;em&gt;sample&lt;/em&gt; instead of &lt;em&gt;dream_number&lt;/em&gt;, then recalculate the most frequently occurring words as a proportion of words by sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_word %&amp;gt;%
  group_by(sample, word) %&amp;gt;%
  summarise(n = n()) %&amp;gt;%
  mutate(percent = (n / sum(n))*100) %&amp;gt;%
  mutate(percent = round(percent, 2)) %&amp;gt;%
  arrange(desc(percent)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,118 x 4
##    sample        word         n percent
##    &amp;lt;fct&amp;gt;         &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 college_women remember    55    1.63
##  2 hall_male     dream       57    1.31
##  3 hall_male     car         51    1.17
##  4 hall_male     house       49    1.13
##  5 vietnam_vet   woman       64    1   
##  6 hall_female   remember    40    0.96
##  7 college_women car         32    0.95
##  8 college_women dream       32    0.95
##  9 hall_female   dream       38    0.92
## 10 hall_female   house       36    0.87
## # … with 8,108 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that for the college women, the word ‘remember’ features the most frequently across the whole sample of 100 dreams and makes up roughly 1.6% of the non-stopwords in the dreams recorded.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;We have explored how to tokenise texts, do some basic text cleaning and creating counts and proportions and finally graphed the simple word counts. In the next post in this series, I will explore the dream data using a clever technique from Julia Silge and David Robinson’s book that involves the &lt;em&gt;spread()&lt;/em&gt; and &lt;em&gt;gather()&lt;/em&gt; functions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>